# Основные сущности Kafka
- Broker
- Zookeeper
- Message (Record)
- Topic/Partition
- Producer
- Consumer

## Broker
- Broker | Kafka Server | Kafka Node
### Функции брокера
- приём сообщений от продюссеров
- хранение данных
- выдача сообщений консюмерам
#### проблемы
- Если всего 1 брокер => проблема надёжности данных,
    производительности
- Решение - создание доп брокеров, образующих кластер
    можем масштабировать и делать репликации

## Zookeeper
- Координатор
- небольшая бд, быстрый на чтение, медленнее на запись
### Функции
- Состояние кластера
    хранение метаданных, состояний, какие есть брокеры, где они
- Конфигурация
- Адресная книга
    где расположены данные; топики, партиции
- Выбор контроллера из Брокеров

## Message
- Важно, чтобы и продюссеры, и консюмеры работали на одной версии, на ко-й стоит Kafka
    пример продюссер отправляет мес-и с хэдерами, а кон-ы не могут прочитать эти мес-и
### Поля сообщений
- Key: ключ используется для распределения сообщений по кластеру
- Value: содержимое сообщения - массив байт
- Timestamp: время мес-а (от эпохи). Уст-я при отправке или обработке внутри кластера
- Headers: набор key-value пар с пользовательскими атрибутами сообщения

## Topic
- поток (стрим) данных однотипных событий(сообщений) FIFO
- данные не удаляются, обеспечивает широковещательный режим
    можно отправлять данные в несколько консюмеров
- данные Топика хранятся в Log-файлах
    Есть Топик А с N кол-вом Партиций, в папке ./logs есть папки под каждую Партицию Топика
    ./A-0; ./A-1; ./A-N
    Внутри ./A-N:
    - 00000000000000.log
        - offset | position | timestamp | message
            - offset: номер сообщения в Партиции
            - position: смещение в байтах до начала нового сообщения
                если есть два сообщения первое занимает от 0 до 66
                второе начинается с 67 байта
    - 00000000000000.index
        - offset | position: маппинг оффсета на позицию
            хотим прочитать с 1 сообщения смотрим позицию
            идём в лог файл делается откат по байтам до позиции первого сообщения
            и затем читаем
    - 00000000000000.timeindex
        - timestamp | offset: маппинг таймстемпа на оффсет
            хотим прочитать что-то со *вторника, но не знаем какой оффсет был во вторник
        - по времени читаем оффсет идём в индекс по оффсету берём позицию - идём в лог
            по позиции берём сообщение
        - могут возникнуть траблы с тем, что выставляем время в Брокерах или данные приходят с
            запозданием и мы как-то время переставляем...
    При превышении 1 Гб log файл закрывается "морозиться" и создаётся новый "сегмент":
    - 00000000000002.log
    - 00000000000002.index
    - 00000000000002.timeindex
    Всегда активен только один сегмент; Имя сегмента строится по первому оффсету попавшему в файл;
    У сегмента есть таймстемп - максимальный таймстемп среди всех сообщений = таймстемп сегмента
    - # Операция удаления данных не поддерживается
        Поддерживается авто удаление данных TTL
        - удаляются целиком сегменты партиций(не отдельные сообщения)
        - segment timestamp expired => to delete
        Если мы наблюдаем, что сегмент накапливается - следует, передано сообщение
            с таймстемпом из будущего
### Partition
- потоки внутри Topic'а, также FIFO
    параллелизация: ускорение чтения/записи данных;
- Если считываем все партиции - неупорядоченно,
    но упорядоченно по Партициям
- #### Проблема
- При распределении Топиков на Брокеры Kafka распределяет Партиции пропорционально
    при автоматическом распределении может получиться, что какой-либо "Жирный" Топик
    будет на одном Брокере, следствием хорошо его нагружать или перег-ть, а маленькие
    Топики будут разбросаны по остальным Брокерам
- Решение: балансировка в ручную, смотрим на нагрузку и раскидываем Партиции по Брокерам
- # Репликация данных
    - replication-factor > 1
        - реплики одной Партиции не могут находится на одном ноде (Брокере)
            При падении одного Брокера Kafka автоматически добавит потерянныче
            реплики с того нода на оставшиеся Брокеры
    - # Проблема оставания реплик
    - # Решение: Master-Slave - гарантия согласованности данных
        - одна из реплик назначается лидером
            остальные становятся "ведомыми" Follower'ами
        - Kafka Controller (особенный Брокер) назначает Leader-реплики
        - # Операции чтения и записи производятся только с Leader-репликой
        - # Проблема: 
            Нужно следить, чтобы Лидер-реплики были разбросаны по Брокерам
            а не содержались на одном, иначе чтение и запись будут производиться
            только через него
            - Можно указать в конфиге, на каком Ноде конкретно должна быть Лидер-реплика
    - # Отставание данных от Лида
        Производится через пуллинг Фолловеров к Лиду
        Фолловеры опрашивают Лидера на наличие новых данных
        - # Кто становится лидером после падения Лидера; И все ли данные есть => Ненадёжно!
            - ISR: Синхронная запись из Leader в ISR Follower(может быть несколько)
                min.insync.replicas = n-1 обычно ставится (n реплик - 1), иначе при падении
                одного Брокера, если у нас всё также будет стоять "n" мы не сможем ничего
                записывать в Топик
                оставшиеся реплики также дальше опрашивают Leader'а и могут отставать
            - ISR Follower - надёжный кандидат на Leader 

## Producer: О том как сообщения в Kafk'у отправить
- ### SEND гарантия и семантика доставки
    - #### acks - гарантия доставки
        - 0 Producer не ждёт подтверждения отправки сообщения
            могут теряться сообщения
        - 1 P ждёт подтверждения только от Leader-реплики,
            если она записала, считает, что сообщение доставлено, перестаёт ждать
            могут теряться соо-я, если Брокер с Лид-репликой упадёт
        - -1 (all) P ждёт подтверждение отправки сообщений от всех ISR-реплик,
            включая Лид-реплику
            *надёжно сообщения не теряются
    - Этапы Сэнда
        - Send message
        - fetch metadata P идёт в zookeeper | В новых версиях Брокер идёт в Zookeeper  
            P должен знать из чего состоит кластер дальше он отправляет
            запрос в Топики, чтобы узнать какие реплики и где находятся являются Лид-ами
            - # "Блокирующая" синхронная операция
                если что-то не так с Zookeeper будет ждать таймаут бай дэфолт 60с 
        - serialize message
            полученное сообщение сериализуем в нужный формат
            key.serializer value.serializer
        - define partition
            - explicit partition: знаем в какую Партицию хотим попасть
            - round-robin: "Разберитесь сами" по кругу-очереди записывает в Партиции
            - key-defined: "ключ сообщения переводим в хэш... берём остаток от деления
                на кол-во партиций, т. к. хэш значение вечное - всегда 
                будем попадать в одну Партицию" 
        - compress message
        - accumulate batch для повышения производительности
            - сообщения отправляются не сразу, мы знаем какой Брокер, какая Партиция
            - batch.size: копим сообщения до переполнения по дэфолту 16кБ
                затем отправляем в Брокер
            - linger.ms: если batch не может переполниться долгое время,
                то по истечению времени отправляем в Брокер
            - Если на одном Брокере находится две и более Лид-Партиции
                batch.size суммируется и при переполнении суммой
                считается, что batch превышен и он отправляется в Брокер

## Consumer О том как получать сообщения из Kafk'и
- также читает только лид-партиции
- ### pool message: получаем сообщения пачками
    - get state & placement
    - Подключение к Лид-репликам всех партиций топика
        может быть медленно
        - # Решение Consumer Group
            указываем для Конс-ов одинаковый <<group.id>>
- ### Topic __consumer_offsets
    - Проблема: мы пуллим из партиции сообшения, он съедает одну пачку,
        получает, обрабатывает и затем отваливается
        обязанности упавшего на себя берёт следующий в группе и заново
        получает и обрабатывает сообщения, уже обработанные первым - мы это не хотим
    - # Хранение оффсетов и коммитов __consumer_offsets (отдельный Топик)
        - хранятся оффсеты для каждой группы, партиции в ситемном Топике Kafk'и
            - fields: Partition | Group | Offset
                - Partition: <Topic_name>/<partition_num>
                - Group: <consumer_group_name> группа, с которой читали
                - Offset: <partition_offset_num> содержит число оффсета, до которого
                    (включая) дочитали
    - # После падения какого-то конс-а
        - консюмер, взявший Партицию упавшего консюмера, читает из __consumer_offsets
            информацию о Партиции <Topic_name>/<partition_num>/<cons_gr_name>/<par_offs_num> 
            и начинает пуллить сообщения со следующего после Offset
        - Таким образом обработка сообщений не дублируется
    - # Типы оффсет коммитов
        - Auto commit: at most once (miss messages)
            - коммитим сразу после получения батча;
                потеря возможна, если консюмер получает сообщения коммитит и
                отваливается, следующий консюмер начинает читать, получать и обрабатывать
                со следующего оффсета
        - Manual commit: at least once (duplicate messages)
            - коммитим только, когда завершена обработка полученных сообщений из батча
                дублирование может происходить из-за того, что, допустим, если 2 из 3 
                сообщений мы обработали, а на 3 отвалились, то коммита не происходит
                и следующий консюмер начинает заново обрабатывать с этого оффсета
        - Custom offset management: exactly once
            - храним оффсеты у себя и спрашиваем брокера со своих оффсетов
    - # Consumer Group не активна долгое время
        При долго инактиве группы информация об оффсетах группы удаляется из Топика
        - max inactive period:
            - offsets.retention.minutes(7 days) # версия 2.1
        - offsets deleted from __consumer_offsets
        - after group activation
            - used: auto.offset reset
                - ealiest: начинаем читать с первого сообщения, которое доступно в Топике
                - latest: начинаем читать с последнего добавленного сообщения

# Почему Kafka быстрая
- масштабируемая архитектура
- последовательные чтение и запись
- кэшируется последние страницы файлов просмотренные консюмерами
- zero-copy
- большие возможности настройки для разных кейсов
- лям сообщений/секунду